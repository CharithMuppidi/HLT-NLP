{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiS6LQdnJKGlCPre0TfJPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoStracts/HLT-NLP/blob/main/Exploring_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Exploring NLTK\n",
        "## Charith Muppidi - srm190013\n"
      ],
      "metadata": {
        "id": "PYBja990jAJK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiz5YpiWLHu7",
        "outputId": "c10eec04-10e4-4da7-c7f7-7406fa95a356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/genesis.zip.\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokens() method functions more as an attribute caller which is why to call it you have to write like .tokens. Throughtout the object class, attributes are private instance variables, as denotes by the underscores, to method calls are used to access them."
      ],
      "metadata": {
        "id": "MGAQt_74PBH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import text1\n",
        "text1.tokens[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi7i3IEmQDr-",
        "outputId": "cbeec6dc-a58a-4bbf-e0b5-e70b2ed8d05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Moby',\n",
              " 'Dick',\n",
              " 'by',\n",
              " 'Herman',\n",
              " 'Melville',\n",
              " '1851',\n",
              " ']',\n",
              " 'ETYMOLOGY',\n",
              " '.',\n",
              " '(',\n",
              " 'Supplied',\n",
              " 'by',\n",
              " 'a',\n",
              " 'Late',\n",
              " 'Consumptive',\n",
              " 'Usher',\n",
              " 'to',\n",
              " 'a',\n",
              " 'Grammar']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From nltk.book we are getting text1 \"Moby Dick\" by Herman Melville, and displaying the tokens from indexes 0 to 19."
      ],
      "metadata": {
        "id": "x-WpLnUXUnzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1.concordance('sea',lines=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj_oywIDWjWj",
        "outputId": "90b6c0af-e5d5-41a1-c248-8e58635161f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 5 of 455 matches:\n",
            " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
            " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
            "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
            "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
            " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use concordance to find the first 5 lines that contain the word 'sea' and display them, aligning the 'sea's with each other. "
      ],
      "metadata": {
        "id": "eCnmVxPaW3C9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There really is not much of a difference between the two Count() methods since the Text objects count() method works by making the tokens attribute call Python's count() method."
      ],
      "metadata": {
        "id": "XZfqyIwiYbVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 'Sea'\n",
        "print(text1.count('sea'))\n",
        "print(text1.count(\"Sea\"))\n",
        "print(text1.count(a))\n",
        "print(text1.tokens.count('sea'))\n",
        "print(text1.tokens.count(\"Sea\"))\n",
        "print(text1.tokens.count(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VlBYVPuXl45",
        "outputId": "b718b1a6-f4de-4b20-99d7-3f1822045b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "433\n",
            "22\n",
            "22\n",
            "433\n",
            "22\n",
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am using Python's and Text's count() method on the word sea to see if there is any difference between the two. The count() methods are returning the number of times the token appears in the text."
      ],
      "metadata": {
        "id": "i48VV3QraPhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Franklin Delano Roosevelt's Inaugural Address, March 04, 1933\n",
        "raw_text = \"\"\"I am certain that my fellow Americans expect that on my induction into the Presidency \n",
        "I will address them with a candor and a decision which the present situation of our nation impels.  \n",
        "This is preeminently the time to speak the truth, the whole truth, frankly and boldly.  \n",
        "Nor need we shrink from honestly facing conditions in our country today.  \n",
        "This great nation will endure as it has endured, will revive and will prosper.  \n",
        "So, first of all, let me assert my firm belief that the only thing we have to fear is fear itself -- \n",
        "nameless, unreasoning unjustified terror which paralyzes needed efforts to convert retreat into advance.  \n",
        "In every dark hour of our national life a leadership of frankness and vigor has met with that understanding and \n",
        "support of the people themselves, which is essential to victory.  \n",
        "I am convinced that you will again give that support to leadership in these critical days.\"\"\"\n",
        "tokens = nltk.word_tokenize(raw_text)\n",
        "print(tokens[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiYYXhPhbJef",
        "outputId": "f4ed6b34-8b3b-41b6-d80d-d6acf34e9ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'certain', 'that', 'my', 'fellow', 'Americans', 'expect', 'that', 'on']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I took the first five sentences in FDR's first Inaugural addres, then I tokenized each words and printed the first 10 of them."
      ],
      "metadata": {
        "id": "7YVpMbN2fnyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(raw_text)\n",
        "[t for t in sentences]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeeqDp3jf7Qy",
        "outputId": "480c53fa-8fa3-4fbf-e6f5-5c9932607e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am certain that my fellow Americans expect that on my induction into the Presidency \\nI will address them with a candor and a decision which the present situation of our nation impels.',\n",
              " 'This is preeminently the time to speak the truth, the whole truth, frankly and boldly.',\n",
              " 'Nor need we shrink from honestly facing conditions in our country today.',\n",
              " 'This great nation will endure as it has endured, will revive and will prosper.',\n",
              " 'So, first of all, let me assert my firm belief that the only thing we have to fear is fear itself -- \\nnameless, unreasoning unjustified terror which paralyzes needed efforts to convert retreat into advance.',\n",
              " 'In every dark hour of our national life a leadership of frankness and vigor has met with that understanding and \\nsupport of the people themselves, which is essential to victory.',\n",
              " 'I am convinced that you will again give that support to leadership in these critical days.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time I am tokenizing and printing the sentences. "
      ],
      "metadata": {
        "id": "JkEV_d4SgzJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemer = nltk.PorterStemmer()\n",
        "[stemer.stem(t) for t in tokens]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyP93aNGhC9b",
        "outputId": "6d0c9c66-1048-4726-97b9-53981ab41e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'am',\n",
              " 'certain',\n",
              " 'that',\n",
              " 'my',\n",
              " 'fellow',\n",
              " 'american',\n",
              " 'expect',\n",
              " 'that',\n",
              " 'on',\n",
              " 'my',\n",
              " 'induct',\n",
              " 'into',\n",
              " 'the',\n",
              " 'presid',\n",
              " 'i',\n",
              " 'will',\n",
              " 'address',\n",
              " 'them',\n",
              " 'with',\n",
              " 'a',\n",
              " 'candor',\n",
              " 'and',\n",
              " 'a',\n",
              " 'decis',\n",
              " 'which',\n",
              " 'the',\n",
              " 'present',\n",
              " 'situat',\n",
              " 'of',\n",
              " 'our',\n",
              " 'nation',\n",
              " 'impel',\n",
              " '.',\n",
              " 'thi',\n",
              " 'is',\n",
              " 'preemin',\n",
              " 'the',\n",
              " 'time',\n",
              " 'to',\n",
              " 'speak',\n",
              " 'the',\n",
              " 'truth',\n",
              " ',',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'truth',\n",
              " ',',\n",
              " 'frankli',\n",
              " 'and',\n",
              " 'boldli',\n",
              " '.',\n",
              " 'nor',\n",
              " 'need',\n",
              " 'we',\n",
              " 'shrink',\n",
              " 'from',\n",
              " 'honestli',\n",
              " 'face',\n",
              " 'condit',\n",
              " 'in',\n",
              " 'our',\n",
              " 'countri',\n",
              " 'today',\n",
              " '.',\n",
              " 'thi',\n",
              " 'great',\n",
              " 'nation',\n",
              " 'will',\n",
              " 'endur',\n",
              " 'as',\n",
              " 'it',\n",
              " 'ha',\n",
              " 'endur',\n",
              " ',',\n",
              " 'will',\n",
              " 'reviv',\n",
              " 'and',\n",
              " 'will',\n",
              " 'prosper',\n",
              " '.',\n",
              " 'so',\n",
              " ',',\n",
              " 'first',\n",
              " 'of',\n",
              " 'all',\n",
              " ',',\n",
              " 'let',\n",
              " 'me',\n",
              " 'assert',\n",
              " 'my',\n",
              " 'firm',\n",
              " 'belief',\n",
              " 'that',\n",
              " 'the',\n",
              " 'onli',\n",
              " 'thing',\n",
              " 'we',\n",
              " 'have',\n",
              " 'to',\n",
              " 'fear',\n",
              " 'is',\n",
              " 'fear',\n",
              " 'itself',\n",
              " '--',\n",
              " 'nameless',\n",
              " ',',\n",
              " 'unreason',\n",
              " 'unjustifi',\n",
              " 'terror',\n",
              " 'which',\n",
              " 'paralyz',\n",
              " 'need',\n",
              " 'effort',\n",
              " 'to',\n",
              " 'convert',\n",
              " 'retreat',\n",
              " 'into',\n",
              " 'advanc',\n",
              " '.',\n",
              " 'in',\n",
              " 'everi',\n",
              " 'dark',\n",
              " 'hour',\n",
              " 'of',\n",
              " 'our',\n",
              " 'nation',\n",
              " 'life',\n",
              " 'a',\n",
              " 'leadership',\n",
              " 'of',\n",
              " 'frank',\n",
              " 'and',\n",
              " 'vigor',\n",
              " 'ha',\n",
              " 'met',\n",
              " 'with',\n",
              " 'that',\n",
              " 'understand',\n",
              " 'and',\n",
              " 'support',\n",
              " 'of',\n",
              " 'the',\n",
              " 'peopl',\n",
              " 'themselv',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'essenti',\n",
              " 'to',\n",
              " 'victori',\n",
              " '.',\n",
              " 'i',\n",
              " 'am',\n",
              " 'convinc',\n",
              " 'that',\n",
              " 'you',\n",
              " 'will',\n",
              " 'again',\n",
              " 'give',\n",
              " 'that',\n",
              " 'support',\n",
              " 'to',\n",
              " 'leadership',\n",
              " 'in',\n",
              " 'these',\n",
              " 'critic',\n",
              " 'day',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using the PorterStemmer to stem the words in the tokens list."
      ],
      "metadata": {
        "id": "LmihmDW9ivw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i-I \n",
        "\n",
        "presid-Presidency\n",
        "\n",
        "decis-decision\n",
        "\n",
        "situat-situation\n",
        "\n",
        "impel-impels\n"
      ],
      "metadata": {
        "id": "WMwQPnLHlP1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lmr = nltk.WordNetLemmatizer()\n",
        "[lmr.lemmatize(t) for t in tokens]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQFLt16Ei372",
        "outputId": "b9ab12a9-548f-42c7-ec6c-82938d6d4905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'am',\n",
              " 'certain',\n",
              " 'that',\n",
              " 'my',\n",
              " 'fellow',\n",
              " 'Americans',\n",
              " 'expect',\n",
              " 'that',\n",
              " 'on',\n",
              " 'my',\n",
              " 'induction',\n",
              " 'into',\n",
              " 'the',\n",
              " 'Presidency',\n",
              " 'I',\n",
              " 'will',\n",
              " 'address',\n",
              " 'them',\n",
              " 'with',\n",
              " 'a',\n",
              " 'candor',\n",
              " 'and',\n",
              " 'a',\n",
              " 'decision',\n",
              " 'which',\n",
              " 'the',\n",
              " 'present',\n",
              " 'situation',\n",
              " 'of',\n",
              " 'our',\n",
              " 'nation',\n",
              " 'impels',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'preeminently',\n",
              " 'the',\n",
              " 'time',\n",
              " 'to',\n",
              " 'speak',\n",
              " 'the',\n",
              " 'truth',\n",
              " ',',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'truth',\n",
              " ',',\n",
              " 'frankly',\n",
              " 'and',\n",
              " 'boldly',\n",
              " '.',\n",
              " 'Nor',\n",
              " 'need',\n",
              " 'we',\n",
              " 'shrink',\n",
              " 'from',\n",
              " 'honestly',\n",
              " 'facing',\n",
              " 'condition',\n",
              " 'in',\n",
              " 'our',\n",
              " 'country',\n",
              " 'today',\n",
              " '.',\n",
              " 'This',\n",
              " 'great',\n",
              " 'nation',\n",
              " 'will',\n",
              " 'endure',\n",
              " 'a',\n",
              " 'it',\n",
              " 'ha',\n",
              " 'endured',\n",
              " ',',\n",
              " 'will',\n",
              " 'revive',\n",
              " 'and',\n",
              " 'will',\n",
              " 'prosper',\n",
              " '.',\n",
              " 'So',\n",
              " ',',\n",
              " 'first',\n",
              " 'of',\n",
              " 'all',\n",
              " ',',\n",
              " 'let',\n",
              " 'me',\n",
              " 'assert',\n",
              " 'my',\n",
              " 'firm',\n",
              " 'belief',\n",
              " 'that',\n",
              " 'the',\n",
              " 'only',\n",
              " 'thing',\n",
              " 'we',\n",
              " 'have',\n",
              " 'to',\n",
              " 'fear',\n",
              " 'is',\n",
              " 'fear',\n",
              " 'itself',\n",
              " '--',\n",
              " 'nameless',\n",
              " ',',\n",
              " 'unreasoning',\n",
              " 'unjustified',\n",
              " 'terror',\n",
              " 'which',\n",
              " 'paralyzes',\n",
              " 'needed',\n",
              " 'effort',\n",
              " 'to',\n",
              " 'convert',\n",
              " 'retreat',\n",
              " 'into',\n",
              " 'advance',\n",
              " '.',\n",
              " 'In',\n",
              " 'every',\n",
              " 'dark',\n",
              " 'hour',\n",
              " 'of',\n",
              " 'our',\n",
              " 'national',\n",
              " 'life',\n",
              " 'a',\n",
              " 'leadership',\n",
              " 'of',\n",
              " 'frankness',\n",
              " 'and',\n",
              " 'vigor',\n",
              " 'ha',\n",
              " 'met',\n",
              " 'with',\n",
              " 'that',\n",
              " 'understanding',\n",
              " 'and',\n",
              " 'support',\n",
              " 'of',\n",
              " 'the',\n",
              " 'people',\n",
              " 'themselves',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'essential',\n",
              " 'to',\n",
              " 'victory',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'convinced',\n",
              " 'that',\n",
              " 'you',\n",
              " 'will',\n",
              " 'again',\n",
              " 'give',\n",
              " 'that',\n",
              " 'support',\n",
              " 'to',\n",
              " 'leadership',\n",
              " 'in',\n",
              " 'these',\n",
              " 'critical',\n",
              " 'day',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yh8-LTXqkr2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are using WordNetLemmatizer to lemmatize the words."
      ],
      "metadata": {
        "id": "Gxx5VvETnYnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this a Python library, the code is top tier. The NLTK library is very functional: there are so many uses for this. I am thinking of using this library to stemming words to find root words. I will also use a lemmatizer to find words that are found in the dictionary. With these to processes, I can compare root/base words with versions with prefixes and suffixes, and I can analyze sound changes in that occur in specific letters. "
      ],
      "metadata": {
        "id": "J8A5l2AXoSLO"
      }
    }
  ]
}